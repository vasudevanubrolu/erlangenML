{
  
    
        "post0": {
            "title": "Foundations of Supervised Learning",
            "content": "Why Does Supervised Learning Work? . We are going to dive deeper into why supevised learning really works. . Part 1: Data Distribution . First, let&#39;s look at the data, and define where it comes from. . this will be useful to precisely define when supervised learning is guaranteed to work. . Review: Components of A Supervised Machine Learning Problem . At a high level, a supervised machine learning problem has the following structure: . $$ underbrace{ text{Training Dataset}}_ text{Attributes + Features} + underbrace{ text{Learning Algorithm}}_ text{Model Class + Objective + Optimizer } to text{Predictive Model} $$ . Where does the dataset come from? . Data Distribution . We will assume that the dataset is sampled from a probability distribution $ mathbb{P}$, which we will call the data distribution. We will denote this as $$x, y sim mathbb{P}.$$ . The training set $ mathcal{D} = {(x^{(i)}, y^{(i)}) mid i = 1,2,...,n }$ consists of independent and identicaly distributed (IID) samples from $ mathbb{P}$. . Data Distribution: IID Sampling . The key assumption in that the training examples are independent and identicaly distributed (IID). . Each training example is from the same distribution. | This distribution doesn&#39;t depend on previous training examples. | . Example: Flipping a coin. Each flip has same probability of heads &amp; tails and doesn&#39;t depend on previous flips. . Counter-Example: Yearly census data. The population in each year will be close to that of the previous year. . Data Distribution: Example . Let&#39;s implement an example of a data distribution in numpy. . import numpy as np np.random.seed(0) def true_fn(X): return np.cos(1.5 * np.pi * X) . Let&#39;s visualize it. . import matplotlib.pyplot as plt plt.rcParams[&#39;figure.figsize&#39;] = [12, 4] X_test = np.linspace(0, 1, 100) plt.plot(X_test, true_fn(X_test), label=&quot;True function&quot;) plt.legend() . &lt;matplotlib.legend.Legend at 0x120e92668&gt; . Let&#39;s now draw samples from the distribution. We will generate random $x$, and then generate random $y$ using $$ y = f(x) + epsilon $$ for a random noise variable $ epsilon$. . n_samples = 30 X = np.sort(np.random.rand(n_samples)) y = true_fn(X) + np.random.randn(n_samples) * 0.1 . We can visualize the samples. . plt.plot(X_test, true_fn(X_test), label=&quot;True function&quot;) plt.scatter(X, y, edgecolor=&#39;b&#39;, s=20, label=&quot;Samples&quot;) plt.legend() . &lt;matplotlib.legend.Legend at 0x12111c860&gt; . Data Distribution: Motivation . Why assume that the dataset is sampled from a distribution? . There is inherent uncertainty in the data. The data may consist of noisy measurements (readings from an imperfect thermometer). | . There is uncertainty in the process we model. If $y$ is a stock price, there is randomness in the market that cannot be modeled. | . We can use probability and statistics to analyze supervised learning algorithms and prove that they work. | . Part 2: Why Does Supervised Learning Work? . We made the assumption that the training dataset is sampled from a data distribution. . Let&#39;s now use it to gain intuition about why supervised learning works. . Review: Data Distribution . We will assume that the dataset is sampled from a probability distribution $ mathbb{P}$, which we will call the data distribution. We will denote this as $$x, y sim mathbb{P}.$$ . The training set $ mathcal{D} = {(x^{(i)}, y^{(i)}) mid i = 1,2,...,n }$ consists of independent and identicaly distributed (IID) samples from $ mathbb{P}$. . Review: Supervised Learning Model . We&#39;ll say that a model is a function $$ f : mathcal{X} to mathcal{Y} $$ that maps inputs $x in mathcal{X}$ to targets $y in mathcal{Y}$. . What Makes A Good Model? . There are several things we may want out of a good model: . Interpretable features that explain how $x$ affects $y$. | Confidence intervals around $y$ (we will see later how to obtain these) | Accurate predictions of the targets $y$ from inputs $x$. | In this lecture, we fill focus on the latter. . What Makes A Good Model? . A good predictive model is one that makes accurate predictions on new data that it has not seen at training time. . Hold-Out Dataset: Definition . A hold-out dataset $$ dot{ mathcal{D}} = {( dot{x}^{(i)}, dot{y}^{(i)}) mid i = 1,2,...,m }$$ is another dataset that is sampled IID from the same distribution $ mathbb{P}$ as the training dataset $ mathcal{D}$ and the two datasets are disjoint. . Let&#39;s genenerate a hold-out dataset for the example we saw earlier. . import numpy as np import matplotlib.pyplot as plt np.random.seed(0) def true_fn(X): return np.cos(1.5 * np.pi * X) X_test = np.linspace(0, 1, 100) plt.plot(X_test, true_fn(X_test), label=&quot;True function&quot;) plt.legend() . &lt;matplotlib.legend.Legend at 0x12116be48&gt; . Let&#39;s genenerate a hold-out dataset for the example we saw earlier. . n_samples, n_holdout_samples = 30, 30 X = np.sort(np.random.rand(n_samples)) y = true_fn(X) + np.random.randn(n_samples) * 0.1 X_holdout = np.sort(np.random.rand(n_holdout_samples)) y_holdout = true_fn(X_holdout) + np.random.randn(n_holdout_samples) * 0.1 plt.plot(X_test, true_fn(X_test), label=&quot;True function&quot;) plt.scatter(X, y, edgecolor=&#39;b&#39;, s=20, label=&quot;Samples&quot;) plt.scatter(X_holdout, y_holdout, edgecolor=&#39;r&#39;, s=20, label=&quot;Holdout Samples&quot;) plt.legend() . &lt;matplotlib.legend.Legend at 0x121440f28&gt; . Defining What is an Accurate Prediction . Suppose that we have a function $ texttt{isaccurate}(y, y&#39;)$ that determines if $y$ is an accurate estimate of $y&#39;$, e.g.: . Is the the target variable close enough to the true target? $$ texttt{isaccurate}(y, y&#39;) = text{true if } (|y - y&#39;| text{ is small), else false}$$ | . Did we predict the right class? $$ texttt{isaccurate}(y, y&#39;) = text{true if } (y = y&#39;) text{ else false} $$ | . This defines accuracy on a data point. We say a supervised learning model is accurate if it correctly predicts the target on new (held-out) data. . Defining What is an Accurate Model . We can say that a predictive model $f$ is accurate if it&#39;s probability of making an error on a random holdout sample is small: . $$ 1 - mathbb{P} left[ texttt{isaccurate}( dot y, f( dot x)) right] leq epsilon $$ . for $ dot{x}, dot{y} sim mathbb{P}$, for some small $ epsilon &gt; 0$ and some definition of accuracy. . We can also say that a predictive model $f$ is inaccurate if it&#39;s probability of making an error on a random holdout sample is large: . $$ 1 - mathbb{P} left[ texttt{isaccurate}( dot y, f( dot x)) right] geq epsilon $$ . or equivalently . $$ mathbb{P} left[ texttt{isaccurate}( dot y, f( dot x)) right] leq 1- epsilon.$$ . Generalization . In machine learning, generalization is the property of predictive models to achieve good performance on new, heldout data that is distinct from the training set. . Will supervised learning return a model that generalizes? . Recall: Supervised Learning . Recall our intuitive definition of supervised learning. . First, we collect a dataset of labeled training examples. | We train a model to output accurate predictions on this dataset. | When the model sees new, similar data, it will also be accurate. | Recall: Supervised Learning . Recall that supervised learning at a high level performs the following procedure: . Collect a training dataset $ mathcal{D}$ of labeled examples. | Output a model that is accurate on $ mathcal{D}$. | I claim that the output model is also guaranteed to generalize if $ mathcal{D}$ is large enough. . Applying Supervised Learning . In order to prove that supervised learning works, we will make two simplifying assumptions: . We define a model class $ mathcal{M}$ containing $H$ different models. | One of these models fits the training data perfectly (is accurate on every point) and we choose that model. | (Both of these assumptions can be relaxed.) . Why Supervised Learning Works . Claim: The probability that supervised learning will return an inaccurate model decreases exponentially with training set size $n$. . A model $f$ is inaccurate if $ mathbb{P} left[ texttt{isaccurate}( dot y, f( dot x)) right] leq 1- epsilon$. The probability that an inaccurate model $f$ perfectly fits the training set is at most $ prod_{i=1}^n mathbb{P} left[ texttt{isaccurate}(y^{(i)}, f(x^{(i)})) right] leq (1- epsilon)^n$. | We have $H$ models in $ mathcal{M}$, and any of them could be in accurate. The probability that at least one the at most $H$ inaccurate models willl fit the training set perfectly is $ leq H (1- epsilon)^n$. | Therefore, the claim holds. . Part 3: Overfitting and Underfitting . Let&#39;s now dive deeper into the concept of generalization and two possible failure modes of supervised learning: overfitting and underfitting. . Review: Generalization . We will assume that the dataset is governed by a probability distribution $ mathbb{P}$, which we will call the data distribution. We will denote this as $$x, y sim mathbb{P}.$$ . A hold-out set $ dot{ mathcal{D}} = {( dot{x^{(i)}}, dot{y^{(i)}}) mid i = 1,2,...,n }$ consists of independent and identicaly distributed (IID) samples from $ mathbb{P}$ and is distinct from the training set. . A model that generalizes is accurate on a hold-out set. . Review: Polynomial Regression . In 1D polynomial regression, we fit a model $$ f_ theta(x) := theta^ top phi(x) $$ that is linear in $ theta$ but non-linear in $x$ because the features $ phi(x) : mathbb{R} to mathbb{R}^p$ are non-linear. . By using polynomial features such as $ phi(x) = [1 ; x ; ldots ; x^p]$, we can fit any polynomial of degree $p$. . Polynomials Better Fit the Data . When we switch from linear models to polynomials, we can better fit the data and increase the accuracy of our models. . Consider the synthetic dataset that we have seen earlier. . from sklearn.pipeline import Pipeline from sklearn.preprocessing import PolynomialFeatures from sklearn.linear_model import LinearRegression np.random.seed(0) n_samples = 30 X = np.sort(np.random.rand(n_samples)) y = true_fn(X) + np.random.randn(n_samples) * 0.1 X_test = np.linspace(0, 1, 100) plt.plot(X_test, true_fn(X_test), label=&quot;True function&quot;) plt.scatter(X, y, edgecolor=&#39;b&#39;, s=20, label=&quot;Samples&quot;) . &lt;matplotlib.collections.PathCollection at 0x12e0c58d0&gt; . Although fitting a linear model does not work well, qudratic or cubic polynomials improve the fit. . degrees = [1, 2, 3] plt.figure(figsize=(14, 5)) for i in range(len(degrees)): ax = plt.subplot(1, len(degrees), i + 1) polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False) linear_regression = LinearRegression() pipeline = Pipeline([(&quot;pf&quot;, polynomial_features), (&quot;lr&quot;, linear_regression)]) pipeline.fit(X[:, np.newaxis], y) ax.plot(X_test, true_fn(X_test), label=&quot;True function&quot;) ax.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=&quot;Model&quot;) ax.scatter(X, y, edgecolor=&#39;b&#39;, s=20, label=&quot;Samples&quot;) ax.set_xlim((0, 1)) ax.set_ylim((-2, 2)) ax.legend(loc=&quot;best&quot;) ax.set_title(&quot;Polynomial of Degree {}&quot;.format(degrees[i])) . Towards Higher-Degree Polynomial Features? . As we increase the complexity of our model class $ mathcal{M}$ to even higher degree polynomials, we are able to fit the data increasingly even better. . What happens if we further increase the degree of the polynomial? . degrees = [30] plt.figure(figsize=(14, 5)) for i in range(len(degrees)): ax = plt.subplot(1, len(degrees), i + 1) polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False) linear_regression = LinearRegression() pipeline = Pipeline([(&quot;pf&quot;, polynomial_features), (&quot;lr&quot;, linear_regression)]) pipeline.fit(X[:, np.newaxis], y) X_test = np.linspace(0, 1, 100) ax.plot(X_test, true_fn(X_test), label=&quot;True function&quot;) ax.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=&quot;Model&quot;) ax.scatter(X, y, edgecolor=&#39;b&#39;, s=20, label=&quot;Samples&quot;) ax.set_xlim((0, 1)) ax.set_ylim((-2, 2)) ax.legend(loc=&quot;best&quot;) ax.set_title(&quot;Polynomial of Degree {}&quot;.format(degrees[i])) . The Problem With Increasing Model Capacity . As the degree of the polynomial increases to the size of the dataset, we are increasingly able to fit every point in the dataset. . However, this results in a highly irregular curve: its behavior outside the training set is wildly inaccurate. . Overfitting . Overfitting is one of the most common failure modes of machine learning. . A very expressive model (a high degree polynomial) fits the training dataset perfectly. | The model also makes wildly incorrect prediction outside this dataset, and doesn&#39;t generalize. | . Underfitting . A related failure mode is underfitting. . A small model (e.g. a straight line), will not fit the training data well. | Held-out data is similar to training data, so it will not be accurate either. | . Finding the tradeoff between overfitting and underfitting is one of the main challenges in applying machine learning. . Overfitting vs. Underfitting: Evaluation . We can measure overfitting and underfitting by estimating accuracy on held-out data and comparing it to the training data. . If training perforance is high but held-out performance is low, we are overfitting. | If training perforance is low but held-out performance is low, we are underfitting. | . degrees = [1, 20, 5] titles = [&#39;Underfitting&#39;, &#39;Overfitting&#39;, &#39;A Good Fit&#39;] plt.figure(figsize=(14, 5)) for i in range(len(degrees)): ax = plt.subplot(1, len(degrees), i + 1) polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False) linear_regression = LinearRegression() pipeline = Pipeline([(&quot;pf&quot;, polynomial_features), (&quot;lr&quot;, linear_regression)]) pipeline.fit(X[:, np.newaxis], y) ax.plot(X_test, true_fn(X_test), label=&quot;True function&quot;) ax.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=&quot;Model&quot;) ax.scatter(X, y, edgecolor=&#39;b&#39;, s=20, label=&quot;Samples&quot;, alpha=0.2) ax.scatter(X_holdout[::3], y_holdout[::3], edgecolor=&#39;r&#39;, s=20, label=&quot;Samples&quot;) ax.set_xlim((0, 1)) ax.set_ylim((-2, 2)) ax.legend(loc=&quot;best&quot;) ax.set_title(&quot;{} (Degree {})&quot;.format(titles[i], degrees[i])) ax.text(0.05,-1.7, &#39;Holdout MSE: %.4f&#39; % ((y_holdout-pipeline.predict(X_holdout[:, np.newaxis]))**2).mean()) . Dealing with Underfitting . Balancing overfitting vs. underfitting is a major challenges in applying machine learning. Briefly, here are some approaches: . To fight under-fitting, we may increase our model class to encompass more expressive models. | We may also create richer features for the data that will make the dataset easier to fit. | . Dealing with Overfitting . We will see many ways of dealing with overftting, but here are some ideas: . If we&#39;re overfitting, we may reduce the complexity of our model by reducing the size of $ mathcal{M}$ | We may also modify our objective to penalize complex models that may overfit the data. | . Part 4: Regularization . We will now see a very important way to reduce overfitting regularization. We will also see several important new algorithms. . Review: Generalization . We will assume that the dataset is governed by a probability distribution $ mathbb{P}$, which we will call the data distribution. We will denote this as $$x, y sim mathbb{P}.$$ . A hold-out set $ dot{ mathcal{D}} = {( dot{x^{(i)}}, dot{y^{(i)}}) mid i = 1,2,...,n }$ consists of independent and identicaly distributed (IID) samples from $ mathbb{P}$ and is distinct from the training set. . Review: Overfitting . Overfitting is one of the most common failure modes of machine learning. . A very expressive model (a high degree polynomial) fits the training dataset perfectly. | The model also makes wildly incorrect prediction outside this dataset, and doesn&#39;t generalize. | . We can visualize overfitting by trying to fit a small dataset with a high degree polynomial. . degrees = [30] plt.figure(figsize=(14, 5)) for i in range(len(degrees)): ax = plt.subplot(1, len(degrees), i + 1) polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False) linear_regression = LinearRegression() pipeline = Pipeline([(&quot;pf&quot;, polynomial_features), (&quot;lr&quot;, linear_regression)]) pipeline.fit(X[:, np.newaxis], y) X_test = np.linspace(0, 1, 100) ax.plot(X_test, true_fn(X_test), label=&quot;True function&quot;) ax.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=&quot;Model&quot;) ax.scatter(X, y, edgecolor=&#39;b&#39;, s=20, label=&quot;Samples&quot;) ax.set_xlim((0, 1)) ax.set_ylim((-2, 2)) ax.legend(loc=&quot;best&quot;) ax.set_title(&quot;Polynomial of Degree {}&quot;.format(degrees[i])) . Regularization: Intuition . The idea of regularization is to penalize complex models that may overfit the data. . In the previous example, a less complex would rely less on polynomial terms of high degree. . Regularization: Definition . The idea of regularization is to train models with an augmented objective $J : mathcal{M} to mathbb{R}$ defined over a training dataset $ mathcal{D}$ of size $n$ as $$J(f) = frac{1}{n} sum_{i=1}^n L(y^{(i)}, f(x^{(i)})) + lambda cdot R(f).$$ . Let&#39;s dissect the components of this objective: . $$J(f) = frac{1}{n} sum_{i=1}^n L(y^{(i)}, f(x^{(i)})) + lambda cdot R(f).$$ . A loss function $L(y, f(x))$ such as the mean squared error. | . A regularizer $R : mathcal{M} to mathbb{R}$ that penalizes models that are overly complex. | . A regularization parameter $ lambda &gt; 0$, which controls the strength of the regularizer. | . When the model $f_ theta$ is parametrized by parameters $ theta$, we can also use the following notation: . $$J( theta) = frac{1}{n} sum_{i=1}^n L(y^{(i)}, f_ theta(x^{(i)})) + lambda cdot R( theta).$$ . L2 Regularization: Definition . How can we define a regularizer $R: mathcal{M} to mathbb{R}$ to control the complexity of a model $f in mathcal{M}$? . In the context of linear models $f(x) = theta^ top x$, a widely used approach is L2 regularization, which defines the following objective: $$J( theta) = frac{1}{n} sum_{i=1}^n L(y^{(i)}, theta^ top x^{(i)}) + frac{ lambda}{2} cdot || theta||_2^2.$$ . Let&#39;s dissect the components of this objective. $$J( theta) = frac{1}{n} sum_{i=1}^n L(y^{(i)}, theta^ top x^{(i)}) + frac{ lambda}{2} cdot || theta||_2^2.$$ . The regularizer $R : mathcal{M} to mathbb{R}$ is the function $R( theta) = || theta||_2^2 = sum_{j=1}^d theta_j^2.$ This is also known as the L2 norm of $ theta$. | . The regularizer penalizes large parameters. This prevents us from over-relying on any single feature and penalizes wildly irregular solutions. | . L2 regularization can be used with most models (linear, neural, etc.) | . L2 Regularization for Polynomial Regression . Let&#39;s consider an application to the polynomial model we have seen so far. Given polynomial features $ phi(x)$, we optimize the following objective: $$ J( theta) = frac{1}{2n} sum_{i=1}^n left( y^{(i)} - theta^ top phi(x^{(i)}) right)^2 + frac{ lambda}{2} cdot || theta||_2^2. $$ . We are going to implement regularized and standard polynomial regression on three random training sets sampled from the same distribution. . from sklearn.linear_model import Ridge degrees = [15, 15, 15] plt.figure(figsize=(14, 5)) for idx, i in enumerate(range(len(degrees))): # sample a dataset np.random.seed(idx) n_samples = 30 X = np.sort(np.random.rand(n_samples)) y = true_fn(X) + np.random.randn(n_samples) * 0.1 # fit a least squares model polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False) linear_regression = LinearRegression() pipeline = Pipeline([(&quot;pf&quot;, polynomial_features), (&quot;lr&quot;, linear_regression)]) pipeline.fit(X[:, np.newaxis], y) # fit a Ridge model polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False) linear_regression = Ridge(alpha=0.1) # sklearn uses alpha instead of lambda pipeline2 = Pipeline([(&quot;pf&quot;, polynomial_features), (&quot;lr&quot;, linear_regression)]) pipeline2.fit(X[:, np.newaxis], y) # visualize results ax = plt.subplot(1, len(degrees), i + 1) # ax.plot(X_test, true_fn(X_test), label=&quot;True function&quot;) ax.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=&quot;No Regularization&quot;) ax.plot(X_test, pipeline2.predict(X_test[:, np.newaxis]), label=&quot;L2 Regularization&quot;) ax.scatter(X, y, edgecolor=&#39;b&#39;, s=20, label=&quot;Samples&quot;) ax.set_xlim((0, 1)) ax.set_ylim((-2, 2)) ax.legend(loc=&quot;best&quot;) ax.set_title(&quot;Dataset sample #{}&quot;.format(idx)) . We can show that by usinng small weights, we prevent the model from learning irregular functions. . print(&#39;Non-regularized weights of the polynomial model need to be large to fit every point:&#39;) print(pipeline.named_steps[&#39;lr&#39;].coef_[:4]) print() print(&#39;By regularizing the weights to be small, we force the curve to be more regular:&#39;) print(pipeline2.named_steps[&#39;lr&#39;].coef_[:4]) . Non-regularized weights of the polynomial model need to be large to fit every point: [-3.02370887e+03 1.16528860e+05 -2.44724185e+06 3.20288837e+07] By regularizing the weights to be small, we force the curve to be more regular: [-2.70114811 -1.20575056 -0.09210716 0.44301292] . How to Choose $ lambda$? . In brief, the most common approach is to choose the value of $ lambda$ that results in the best performance on a held-out validation set. . We will later see this strategies and several other in more detail . Normal Equations for Regularized Models . How, do we fit regularized models? In the linear case, we can do this easily by deriving generalized normal equations! . Let $L( theta) = frac{1}{2} (X theta - y)^ top (X theta - y)$ be our least squares objective. We can write the Ridge objective as: $$ J( theta) = frac{1}{2} (X theta - y)^ top (X theta - y) + frac{1}{2} lambda || theta||_2^2 $$ . This allows us to derive the gradient as follows: . $$ begin{aligned} nabla_ theta J( theta) &amp; = nabla_ theta left( frac{1}{2} (X theta - y)^ top (X theta - y) + frac{1}{2} lambda || theta||_2^2 right) &amp; = nabla_ theta left( L( theta) + frac{1}{2} lambda || theta||_2^2 right) &amp; = nabla_ theta L( theta) + lambda theta &amp; = (X^ top X) theta - X^ top y + lambda theta &amp; = (X^ top X + lambda I) theta - X^ top y end{aligned}$$ . We used the derivation of the normal equations for least squares to obtain $ nabla_ theta L( theta)$ as well as the fact that: $ nabla_x x^ top x = 2 x$. . We can set the gradient to zero to obtain normal equations for the Ridge model: $$ (X^ top X + lambda I) theta = X^ top y. $$ . Hence, the value $ theta^*$ that minimizes this objective is given by: $$ theta^* = (X^ top X + lambda I)^{-1} X^ top y.$$ . Note that the matrix $(X^ top X + lambda I)$ is always invertible, which addresses a problem with least squares that we saw earlier. . Algorithm: Ridge Regression . Type: Supervised learning (regression) | Model family: Linear models | Objective function: L2-regularized mean squared error | Optimizer: Normal equations | . Part 5: Regularization and Sparsity . We will now look another form of regularization, which will have an important new property called sparsity. . Regularization: Definition . The idea of regularization is to train models with an augmented objective $J : mathcal{M} to mathbb{R}$ defined over a training dataset $ mathcal{D}$ of size $n$ as $$ J(f) = frac{1}{n} sum_{i=1}^n L(y^{(i)}, f(x^{(i)})) + lambda cdot R(f). $$ . Let&#39;s dissect the components of this objective: . $$ J(f) = frac{1}{n} sum_{i=1}^n L(y^{(i)}, f(x^{(i)})) + lambda cdot R(f). $$ . A loss function $L(y, f(x))$ such as the mean squared error. | . A regularizer $R : mathcal{M} to mathbb{R}$ that penalizes models that are overly complex. | . L1 Regularizion: Definition . Another closely related approach to regularization is to penalize the size of the weights using the L1 norm. . In the context of linear models $f(x) = theta^ top x$, L1 regularization yields the following objective: $$ J( theta) = frac{1}{n} sum_{i=1}^n L(y^{(i)}, theta^ top x^{(i)}) + lambda cdot || theta||_1. $$ . Let&#39;s dissect the components of this objective. $$ J( theta) = frac{1}{n} sum_{i=1}^n L(y^{(i)}, theta^ top x^{(i)}) + lambda cdot || theta||_1. $$ . The regularizer $R : mathcal{M} to mathbb{R}$ is the function $R( theta) = || theta||_1 = sum_{j=1}^d | theta_j|.$ This is also known as the L1 norm of $ theta$. | . The regularizer also penalizes large weights. It also forces more weights to decay to zero, as opposed to just being small. | . Algorithm: Lasso . L1-regularized linear regression is also known as the Lasso (least absolute shrinkage and selection operator). . Type: Supervised learning (regression) | Model family: Linear models | Objective function: L1-regularized mean squared error | . Optimizer: gradient descent, coordinate descent, least angle regression (LARS) and others | . Regularizing via Constraints . Consider regularized problem with a penalty term: $$ min_{ theta in Theta} L( theta) + lambda cdot R( theta). $$ . We may also enforce an explicit constraint on the complexity of the model: . $$ begin{aligned} min_{ theta in Theta} ; &amp; L( theta) such ; that ; &amp; R( theta) leq lambda&#39; end{aligned}$$ . We will not prove this, but solving this problem is equivalent so solving the penalized problem for some $ lambda &gt; 0$ that&#39;s different from $ lambda&#39;$. . In other words, . We can regularize by explicitly enforcing $R( theta)$ to be less than a value instead of penalizing it. | For each value of $ lambda$, we are implicitly setting a constraint of $R( theta)$. | . Regularizing via Constraints: Example . This is what it looks like for a linear model: $$ begin{aligned} min_{ theta in Theta} ; &amp; frac{1}{2n} sum_{i=1}^n left( y^{(i)} - theta^ top x^{(i)} right)^2 such ; that ; &amp; || theta|| leq lambda&#39; end{aligned}$$ . where $|| cdot||$ can either be the L1 or L2 norm. . L1 vs. L2 Regularization . Sparsity: Definition . A vector is said to be sparse if a large fraction of its entires is zero. . L1-regularized linear regression produces sparse weights. . This is makes the model more interpretable | It also makes it computationally more tractable in very large dimensions. | . Sparsity: Ridge Model . To better understand sparsity, we will fit L2-regularized linear models to the UCI diabetes dataset and observe the magnitude of each weight (colored lines) as a function of the regularization parameter. . from sklearn.datasets import load_diabetes from sklearn.linear_model import Ridge from matplotlib import pyplot as plt X, y = load_diabetes(return_X_y=True) # create ridge coefficients alphas = np.logspace(-5, 2, ) ridge_coefs = [] for a in alphas: ridge = Ridge(alpha=a, fit_intercept=False) ridge.fit(X, y) ridge_coefs.append(ridge.coef_) # plot ridge coefficients plt.figure(figsize=(14, 5)) plt.plot(alphas, ridge_coefs) plt.xscale(&#39;log&#39;) plt.xlabel(&#39;Regularization parameter (lambda)&#39;) plt.ylabel(&#39;Magnitude of model parameters&#39;) plt.title(&#39;Ridge coefficients as a function of the regularization&#39;) plt.axis(&#39;tight&#39;) . (4.466835921509635e-06, 223.872113856834, -868.4051623855127, 828.0533448059361) . Sparsity: Lasso Model . The above Ridge model did not produce sparse weights. Let&#39;s now compare it to a Lasso model. . import warnings warnings.filterwarnings(&quot;ignore&quot;) from sklearn.datasets import load_diabetes from sklearn.linear_model import lars_path # create lasso coefficients _, _, lasso_coefs = lars_path(X, y, method=&#39;lasso&#39;) xx = np.sum(np.abs(lasso_coefs.T), axis=1) # plot ridge coefficients plt.figure(figsize=(14, 5)) plt.subplot(&#39;121&#39;) plt.plot(alphas, ridge_coefs) plt.xscale(&#39;log&#39;) plt.ylabel(&#39;Regularization Strength (alpha)&#39;) plt.ylabel(&#39;Coefficents&#39;) plt.title(&#39;Ridge coefficients as a function of the regularization&#39;) plt.axis(&#39;tight&#39;) # plot lasso coefficients plt.subplot(&#39;122&#39;) plt.plot(3500-xx, lasso_coefs.T) ymin, ymax = plt.ylim() plt.xlim(ax.get_xlim()[::-1]) # reverse axis plt.ylabel(&#39;Coefficients&#39;) plt.ylabel(&#39;Regularization Strength&#39;) plt.title(&#39;LASSO Path&#39;) plt.axis(&#39;tight&#39;) . (3673.0002477572816, -133.00520290291772, -869.3573357636973, 828.4524952229636) . &lt;/img&gt; Created in Deepnote .",
            "url": "https://vasudevanubrolu.github.io/erlangenML/2022/02/10/foundations-supervised-learning.html",
            "relUrl": "/2022/02/10/foundations-supervised-learning.html",
            "date": " • Feb 10, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Generative Algorithms",
            "content": "Part 1: Generative Models . We are going to look at generative algorithms and their applications to classification. . We will start by defining the concept of a generative model. . Review: Components of A Supervised Machine Learning Problem . At a high level, a supervised machine learning problem has the following structure: . $$ underbrace{ text{Training Dataset}}_ text{Attributes + Features} + underbrace{ text{Learning Algorithm}}_ text{Model Class + Objective + Optimizer } to text{Predictive Model} $$ . Review: Probabilistic Models . A (parametric) probabilistic model with parameters $ theta$ is a probability distribution $$P_ theta(x,y) : mathcal{X} times mathcal{Y} to [0,1].$$ This model can approximate the data distribution $ mathbb{P}(x,y)$. . If we know $P_ theta(x,y)$, we can compute predictions using the formula $$P_ theta(y|x) = frac{P_ theta(x,y)}{P_ theta(x)} = frac{P_ theta(x,y)}{ sum_{y in mathcal{Y}} P_ theta(x, y)}.$$ . Review: Maximum Likelihood Learning . In order to fit probabilistic models, we use the following objective: $$ max_ theta mathbb{E}_{x, y sim mathbb{P}_ text{data}} log P_ theta(x, y). $$ This seeks to find a model that assigns high probability to the training data. . Review: Conditional Probabilistic Models . Alternatively, we may define a model of the conditional probability distribution: $$P_ theta(y|x) : mathcal{X} times mathcal{Y} to [0,1].$$ . These are trained using conditional maximum likelihood: $$ max_ theta mathbb{E}_{x, y sim mathbb{P}_ text{data}} log P_ theta(y|x). $$ This seeks to find a model that assigns high conditional probability to the target $y$ for each $x$. . Logistic regression is an example of this approach. . Discriminative vs. Generative Models . These two types of models are also known as generative and discriminative. . $$ begin{aligned} underbrace{P_ theta(x,y) : mathcal{X} times mathcal{Y} to [0,1]}_ text{generative model} &amp; ; ; &amp; underbrace{P_ theta(y|x) : mathcal{X} times mathcal{Y} to [0,1]}_ text{discriminative model} end{aligned}$$ . The models parametrize different kinds of probabilities | They involve different training objectives and make different predictions | Their uses are different (e.g., prediction, generation); more later! | . Classification Dataset: Iris Flowers . To demonstrate the two approaches, we are going to use the Iris flower dataset. . import numpy as np import pandas as pd import warnings warnings.filterwarnings(&#39;ignore&#39;) from sklearn import datasets # Load the Iris dataset iris = datasets.load_iris(as_frame=True) # print part of the dataset iris_X, iris_y = iris.data, iris.target pd.concat([iris_X, iris_y], axis=1).head() . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target . 0 5.1 | 3.5 | 1.4 | 0.2 | 0 | . 1 4.9 | 3.0 | 1.4 | 0.2 | 0 | . 2 4.7 | 3.2 | 1.3 | 0.2 | 0 | . 3 4.6 | 3.1 | 1.5 | 0.2 | 0 | . 4 5.0 | 3.6 | 1.4 | 0.2 | 0 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; If we only consider the first two feature columns, we can visualize the dataset in 2D. . %matplotlib inline from matplotlib import pyplot as plt plt.rcParams[&#39;figure.figsize&#39;] = [12, 4] # create 2d version of dataset X = iris_X.to_numpy()[:,:2] x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5 y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5 # Plot also the training points p1 = plt.scatter(X[:, 0], X[:, 1], c=iris_y, edgecolor=&#39;k&#39;, s=60, cmap=plt.cm.Paired) plt.xlabel(&#39;Sepal Length (cm)&#39;) plt.ylabel(&#39;Sepal Width (cm)&#39;) plt.legend(handles=p1.legend_elements()[0], labels=[&#39;Setosa&#39;, &#39;Versicolour&#39;, &#39;Virginica&#39;], loc=&#39;lower right&#39;) . &lt;matplotlib.legend.Legend at 0x7f6f57be2610&gt; . Example: Discriminative Model . An example of a discriminative model is logistic or softmax regression. . Discriminative models directly partition the feature space into regions associated with each class and separated by a decision boundary. | . Given features $x$, discriminative models directly map to predicted classes (e.g., via the function $ sigma( theta^ top x)$ for logistic regression). | . from sklearn.linear_model import LogisticRegression logreg = LogisticRegression(C=1e5, multi_class=&#39;multinomial&#39;) # Create an instance of Softmax and fit the data. logreg.fit(X, iris_y) xx, yy = np.meshgrid(np.arange(x_min, x_max, .02), np.arange(y_min, y_max, .02)) Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()]) # Put the result into a color plot Z = Z.reshape(xx.shape) plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired) # Plot also the training points plt.scatter(X[:, 0], X[:, 1], c=iris_y, edgecolors=&#39;k&#39;, s=60, cmap=plt.cm.Paired) plt.xlabel(&#39;Sepal length&#39;) plt.ylabel(&#39;Sepal width&#39;) . Text(0, 0.5, &#39;Sepal width&#39;) . Example: Generative Model . Generative modeling can be seen as taking a different approach: . In the Iris example, we first build a model of how each type of flower looks, i.e. we can learn the distribution $$ p(x | y=k) ; text{for each class $k$}.$$ It defines a model of how each flower is generated, hence the name. | Given a new flower datapoint $x&#39;$, we can match it against each flower model and find the type of flower that looks most similar to it. Mathematically, this corresponds to: $$ begin{aligned} arg max_y log p(y | x) &amp; = arg max_y log frac{p(x | y) p(y)}{p(x)} &amp; = arg max_y log p(x | y) p(y), end{aligned}$$ | where we have applied Bayes&#39; rule in the first line. . Generative vs. Discriminative Approaches . How do we know which approach is better? . If we only care about prediction, we don&#39;t need a model of $P(x)$. We can solve precisely the problem we care about. Discriminative models will often be more accurate. | . | . If we care about other tasks (generation, dealing with missing values, etc.) or if we know the true model is generative, we want to use the generative approach. | . More on this later! . Part 2: Gaussian Discriminant Analysis . We are now going to continue our discussion of classification. . We will see a new classification algorithm, Gaussian Discriminant Analysis. | This will be our first example of generative machine learning model. | . Review: Classification . Consider a training dataset $ mathcal{D} = {(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ldots, (x^{(n)}, y^{(n)}) }$. . We distinguish between two types of supervised learning problems depnding on the targets $y^{(i)}$. . Regression: The target variable $y in mathcal{Y}$ is continuous: $ mathcal{Y} subseteq mathbb{R}$. | Classification: The target variable $y$ is discrete and takes on one of $K$ possible values: $ mathcal{Y} = {y_1, y_2, ldots y_K }$. Each discrete value corresponds to a class that we want to predict. | Review: Generative Models . There are two types of probabilistic models: generative and discriminative. . $$ begin{aligned} underbrace{P_ theta(x,y) : mathcal{X} times mathcal{Y} to [0,1]}_ text{generative model} &amp; ; ; &amp; underbrace{P_ theta(y|x) : mathcal{X} times mathcal{Y} to [0,1]}_ text{discriminative model} end{aligned}$$ . They involve different training objectives and make different predictions | Their uses are different (e.g., prediction, generation); more later! | . Mixtures of Gaussians . A mixture of $K$ Gaussians is a distribution $P(x)$ of the form: . $$ phi_1 mathcal{N}(x; mu_1, Sigma_1) + phi_2 mathcal{N}(x; mu_2, Sigma_2) + ldots + phi_K mathcal{N}(x; mu_K, Sigma_K).$$ . Each $ mathcal{N}(x; mu_k, Sigma_k)$ is a (multivariate) Gaussian distribution with mean $ mu_k$ and covariance $ Sigma_k$. | The $ phi_k$ are weights, and the above sum is a weighted average of the $K$ Gaussians. | . We can easily visualize this in 1D: . def N(x,mu,sigma): return np.exp(-.5*(x-mu)**2/sigma**2)/np.sqrt(2*np.pi*sigma) def mixture(x): return 0.6*N(x,mu=1,sigma=0.5) + 0.4*N(x,mu=-1,sigma=0.5) xs, xs1, xs2 = np.linspace(-3,3), np.linspace(-1,3), np.linspace(-3,1) plt.subplot(&#39;121&#39;) plt.plot(xs, mixture(xs), label=&#39;Mixture density&#39;, linewidth=2) plt.legend() plt.subplot(&#39;122&#39;) plt.plot(xs1, 0.6*N(xs1,mu=1,sigma=0.5), label=&#39;First Gaussian&#39;, alpha=0.7) plt.plot(xs2, 0.4*N(xs2,mu=-1,sigma=0.5), label=&#39;Second Gaussian&#39;, alpha=0.7) plt.legend() . &lt;matplotlib.legend.Legend at 0x7f6f57267590&gt; . Gaussian Discriminant Model . We may use this approach to define a model $P_ theta$. This will be the basis of an algorthim called Gaussian Discriminant Analysis. . The distribution over classes is Categorical, denoted $ text{Categorical}( phi_1, phi_2, ..., phi_K)$. Thus, $P_ theta(y=k) = phi_k$. | The conditional probability $P_ theta(x mid y=k)$ of the data under class $k$ is a multivariate Gaussian $ mathcal{N}(x; mu_k, Sigma_k)$ with mean and covariance $ mu_k, Sigma_k$. | . Thus, $P_ theta(x,y)$ is a mixture of $K$ Gaussians: $$P_ theta(x,y) = sum_{k=1}^K P_ theta(y=k) P_ theta(x|y=k) = sum_{k=1}^K phi_k mathcal{N}(x; mu_k, Sigma_k)$$ . Intuitively, this model defines a story for how the data was generated. To obtain a data point, . First, we sample a class $y sim text{Categorical}( phi_1, phi_2, ..., phi_K)$ with class proportions given by the $ phi_k$. | Then, we sample an $x$ from a Gaussian distribution $ mathcal{N}( mu_k, Sigma_k)$ specific to that class. | . Such a story can be constructed for most generative algorithms and helps understand them. . Classification Dataset: Iris Flowers . import numpy as np import pandas as pd import warnings warnings.filterwarnings(&#39;ignore&#39;) from sklearn import datasets # Load the Iris dataset iris = datasets.load_iris(as_frame=True) # print part of the dataset iris_X, iris_y = iris.data, iris.target pd.concat([iris_X, iris_y], axis=1).head() . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target . 0 5.1 | 3.5 | 1.4 | 0.2 | 0 | . 1 4.9 | 3.0 | 1.4 | 0.2 | 0 | . 2 4.7 | 3.2 | 1.3 | 0.2 | 0 | . 3 4.6 | 3.1 | 1.5 | 0.2 | 0 | . 4 5.0 | 3.6 | 1.4 | 0.2 | 0 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; If we only consider the first two feature columns, we can visualize the dataset in 2D. . %matplotlib inline from matplotlib import pyplot as plt plt.rcParams[&#39;figure.figsize&#39;] = [12, 4] # create 2d version of dataset X = iris_X.to_numpy()[:,:2] x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5 y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5 # Plot also the training points p1 = plt.scatter(X[:, 0], X[:, 1], c=iris_y, edgecolor=&#39;k&#39;, s=60, cmap=plt.cm.Paired) plt.xlabel(&#39;Sepal Length (cm)&#39;) plt.ylabel(&#39;Sepal Width (cm)&#39;) plt.legend(handles=p1.legend_elements()[0], labels=[&#39;Setosa&#39;, &#39;Versicolour&#39;, &#39;Virginica&#39;], loc=&#39;lower right&#39;) . &lt;matplotlib.legend.Legend at 0x7f6f57028190&gt; . Example: Iris Flower Classification . Let&#39;s see how this approach can be used in practice on the Iris dataset. . We will &quot;guess&quot; a good set of parameters for a Gaussian Discriminant model | We will sample from the model and compare to the true data | . s = 100 # number of samples K = 3 # number of classes d = 2 # number of features # guess the parameters phi = 1./K * np.ones(K,) mus = np.array( [[5.0, 3.5], [6.0, 2.5], [6.5, 3.0]] ) Sigmas = 0.05*np.tile(np.reshape(np.eye(2),(1,2,2)),(K,1,1)) # generate data from this model ys = np.random.multinomial(n=1, pvals=phi, size=(s,)).argmax(axis=1) xs = np.zeros([s,d]) for k in range(K): nk = (ys==k).sum() xs[ys==k,:] = np.random.multivariate_normal(mus[k], Sigmas[k], size=(nk,)) print(xs[:10]) . [[6.1819394 3.26690585] [6.52459241 2.83373478] [5.34563942 3.32322568] [6.33934233 2.56091223] [5.05576182 3.97106613] [5.86814899 2.20227669] [6.59858694 2.96367877] [5.96915307 2.69021308] [6.69117452 3.23679551] [6.23529554 2.75891253]] . plt.subplot(&#39;121&#39;) plt.title(&#39;Model Samples&#39;) plt.scatter(xs[:,0], xs[:,1], c=ys, cmap=plt.cm.Paired) plt.scatter(X[:, 0], X[:, 1], c=iris_y, edgecolors=&#39;k&#39;, cmap=plt.cm.Paired, alpha=0.15) plt.xlabel(&#39;Sepal length&#39;) plt.ylabel(&#39;Sepal width&#39;) # Plot also the training points plt.subplot(&#39;122&#39;) plt.title(&#39;Training Dataset&#39;) plt.scatter(X[:, 0], X[:, 1], c=iris_y, edgecolors=&#39;k&#39;, cmap=plt.cm.Paired, alpha=1) plt.xlabel(&#39;Sepal length&#39;) plt.ylabel(&#39;Sepal width&#39;) . Text(0, 0.5, &#39;Sepal width&#39;) . Our Gaussian Discirminant model generates data that looks not unlike the real data. | Let&#39;s now see how we can learn parameters from data and use the model to make predictions. | . Part 3: Gaussian Discriminant Analysis: Learning . We continue our discussion of Gaussian Discriminant analysis, and look at: . How to learn parameters of the mixture model | How to use the model to make predictions | . Review: Classification . Consider a training dataset $ mathcal{D} = {(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ldots, (x^{(n)}, y^{(n)}) }$. . We distinguish between two types of supervised learning problems depnding on the targets $y^{(i)}$. . Regression: The target variable $y in mathcal{Y}$ is continuous: $ mathcal{Y} subseteq mathbb{R}$. | Classification: The target variable $y$ is discrete and takes on one of $K$ possible values: $ mathcal{Y} = {y_1, y_2, ldots y_K }$. Each discrete value corresponds to a class that we want to predict. | Review: Gaussian Discriminant Model . We may define a model $P_ theta$ as follows. This will be the basis of an algorthim called Gaussian Discriminant Analysis. . The distribution over classes is Categorical, denoted $ text{Categorical}( phi_1, phi_2, ..., phi_K)$. Thus, $P_ theta(y=k) = phi_k$. | The conditional probability $P(x mid y=k)$ of the data under class $k$ is a multivariate Gaussian $ mathcal{N}(x; mu_k, Sigma_k)$ with mean and covariance $ mu_k, Sigma_k$. | . Thus, $P_ theta(x,y)$ is a mixture of $K$ Gaussians: $$P_ theta(x,y) = sum_{k=1}^K P_ theta(y=k) P_ theta(x|y=k) = sum_{k=1}^K phi_k mathcal{N}(x; mu_k, Sigma_k)$$ . Review: Maximum Likelihood Learning . In order to fit probabilistic models, we use the following objective: $$ max_ theta mathbb{E}_{x, y sim mathbb{P}_ text{data}} log P_ theta(x, y). $$ This seeks to find a model that assigns high probability to the training data. . Let&#39;s use maximum likelihood to fit the Guassian Discriminant model. Note that model parameterss $ theta$ are the union of the parameters of each sub-model: $$ theta = ( mu_1, Sigma_1, phi_1, ldots, mu_K, Sigma_K, phi_K).$$ . Mathematically, the components of the model $P_ theta(x,y)$ are as follows. . $$ begin{aligned}P_ theta(y) &amp; = frac{ prod_{k=1}^K phi_k^{ mathbb{I} {y = y_k }}}{ sum_{k=1}^k phi_k} P_ theta(x|y=k) &amp; = frac{1}{(2 pi)^{d/2}| Sigma|^{1/2}} exp(- frac{1}{2}(x- mu_k)^ top Sigma_k^{-1}(x- mu_k)) end{aligned}$$ . Optimizing the Log Likelihood . Given a dataset $ mathcal{D} = {(x^{(i)}, y^{(i)}) mid i=1,2, ldots,n }$, we want to optimize the log-likelihood $ ell( theta)$: $$ begin{aligned} ell( theta) &amp; = sum_{i=1}^n log P_ theta(x^{(i)}, y^{(i)}) = sum_{i=1}^n log P_ theta(x^{(i)} | y^{(i)}) + sum_{i=1}^n log P_ theta(y^{(i)}) &amp; = sum_{k=1}^K underbrace{ sum_{i : y^{(i)} = k} log P(x^{(i)} | y^{(i)} ; mu_k, Sigma_k)}_ text{all the terms that involve $ mu_k, Sigma_k$} + underbrace{ sum_{i=1}^n log P(y^{(i)} ; vec phi)}_ text{all the terms that involve $ vec phi$}. end{aligned}$$ . Notice that each set of parameters $( mu_k, Sigma_k)$ is found in only one term of the summation over the $K$ classes and the $ phi_k$ are also in the same term. . Since each $( mu_k, Sigma_k)$ for $k=1,2, ldots,K$ is found in one term, optimization over $( mu_k, Sigma_k)$ can be carried out independently of all the other parameters by just looking at that term: . $$ begin{aligned} max_{ mu_k, Sigma_k} sum_{i=1}^n log P_ theta(x^{(i)}, y^{(i)})&amp; = max_{ mu_k, Sigma_k} sum_{l=1}^K sum_{i : y^{(i)} = l} log P_ theta(x^{(i)} | y^{(i)} ; mu_l, Sigma_l) &amp; = max_{ mu_k, Sigma_k} sum_{i : y^{(i)} = k} log P_ theta(x^{(i)} | y^{(i)} ; mu_k, Sigma_k). end{aligned}$$ . Similarly, optimizing for $ vec phi = ( phi_1, phi_2, ldots, phi_K)$ only involves a single term: . $$ max_{ vec phi} sum_{i=1}^n log P_ theta(x^{(i)}, y^{(i)} ; theta) = max_{ vec phi} sum_{i=1}^n log P_ theta(y^{(i)} ; vec phi). $$ Optimizing the Class Probabilities . These observations greatly simplify the optimization of the model. Let&#39;s first consider the optimization over $ vec phi = ( phi_1, phi_2, ldots, phi_K)$. From the previous anaylsis, our objective $J( vec phi)$ equals . $$ begin{aligned}J( vec phi) &amp; = sum_{i=1}^n log P_ theta(y^{(i)} ; vec phi) &amp; = sum_{i=1}^n log phi_{y^{(i)}} - n cdot log sum_{k=1}^K phi_k &amp; = sum_{k=1}^K sum_{i : y^{(i)} = k} log phi_k - n cdot log sum_{k=1}^K phi_k end{aligned}$$ . Taking the derivative and setting it to zero, we obtain $$ frac{ phi_k}{ sum_l phi_l} = frac{n_k}{n}$$ for each $k$, where $n_k = | {i : y^{(i)} = k }|$ is the number of training targets with class $k$. . Thus, the optimal $ phi_k$ is just the proportion of data points with class $k$ in the training set! . Optimizing Conditional Probabilities . Similarly, we can maximize the likelihood $$ max_{ mu_k, Sigma_k} sum_{i : y^{(i)} = k} log P(x^{(i)} | y^{(i)} ; mu_k, Sigma_k) = max_{ mu_k, Sigma_k} sum_{i : y^{(i)} = k} log mathcal{N}(x^{(i)} | mu_k, Sigma_k) $$ over the Gaussian parameters. . Computing the derivative and setting it to zero, we obtain closed form solutions: . $$ begin{aligned} mu_k &amp; = frac{ sum_{i: y^{(i)} = k} x^{(i)}}{n_k} Sigma_k &amp; = frac{ sum_{i: y^{(i)} = k} (x^{(i)} - mu_k)(x^{(i)} - mu_k)^ top}{n_k} end{aligned}$$ . These are just the empirical means and covariances of each class. . Querying the Model . How do we ask the model for predictions? As discussed earler, we can apply Bayes&#39; rule: $$ arg max_y P_ theta(y|x) = arg max_y P_ theta(x|y)P(y).$$ Thus, we can estimate the probability of $x$ and under each $P_ theta(x|y=k)P(y=k)$ and choose the class that explains the data best. . Classification Dataset: Iris Flowers . import numpy as np import pandas as pd import warnings warnings.filterwarnings(&#39;ignore&#39;) from sklearn import datasets # Load the Iris dataset iris = datasets.load_iris(as_frame=True) # print part of the dataset iris_X, iris_y = iris.data, iris.target pd.concat([iris_X, iris_y], axis=1).head() . sepal length (cm) sepal width (cm) petal length (cm) petal width (cm) target . 0 5.1 | 3.5 | 1.4 | 0.2 | 0 | . 1 4.9 | 3.0 | 1.4 | 0.2 | 0 | . 2 4.7 | 3.2 | 1.3 | 0.2 | 0 | . 3 4.6 | 3.1 | 1.5 | 0.2 | 0 | . 4 5.0 | 3.6 | 1.4 | 0.2 | 0 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; If we only consider the first two feature columns, we can visualize the dataset in 2D. . %matplotlib inline from matplotlib import pyplot as plt plt.rcParams[&#39;figure.figsize&#39;] = [12, 4] # create 2d version of dataset X = iris_X.to_numpy()[:,:2] x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5 y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5 # Plot also the training points p1 = plt.scatter(X[:, 0], X[:, 1], c=iris_y, edgecolor=&#39;k&#39;, s=60, cmap=plt.cm.Paired) plt.xlabel(&#39;Sepal Length (cm)&#39;) plt.ylabel(&#39;Sepal Width (cm)&#39;) plt.legend(handles=p1.legend_elements()[0], labels=[&#39;Setosa&#39;, &#39;Versicolour&#39;, &#39;Virginica&#39;], loc=&#39;lower right&#39;) . &lt;matplotlib.legend.Legend at 0x7f6f56e85d50&gt; . Example: Iris Flower Classification . Let&#39;s see how this approach can be used in practice on the Iris dataset. . We will learn a good set of parameters for a Gaussian Discriminant model | We will compare the outputs to the true predictions. | . Let&#39;s first start by computing the true parameters on our dataset. . d = 2 # number of features in our toy dataset K = 3 # number of clases n = X.shape[0] # size of the dataset # these are the shapes of the parameters mus = np.zeros([K,d]) Sigmas = np.zeros([K,d,d]) phis = np.zeros([K]) # we now compute the parameters for k in range(3): X_k = X[iris_y == k] mus[k] = np.mean(X_k, axis=0) Sigmas[k] = np.cov(X_k.T) phis[k] = X_k.shape[0] / float(n) # print out the means print(mus) . [[5.006 3.428] [5.936 2.77 ] [6.588 2.974]] . We can compute predictions using Bayes&#39; rule. . def gda_predictions(x, mus, Sigmas, phis): &quot;&quot;&quot;This returns class assignments and p(y|x) under the GDA model. We compute arg max_y p(y|x) as arg max_y p(x|y)p(y) &quot;&quot;&quot; # adjust shapes n, d = x.shape x = np.reshape(x, (1, n, d, 1)) mus = np.reshape(mus, (K, 1, d, 1)) Sigmas = np.reshape(Sigmas, (K, 1, d, d)) # compute probabilities py = np.tile(phis.reshape((K,1)), (1,n)).reshape([K,n,1,1]) pxy = ( np.sqrt(np.abs((2*np.pi)**d*np.linalg.det(Sigmas))).reshape((K,1,1,1)) * -.5*np.exp( np.matmul(np.matmul((x-mus).transpose([0,1,3,2]), np.linalg.inv(Sigmas)), x-mus) ) ) pyx = pxy * py return pyx.argmax(axis=0).flatten(), pyx.reshape([K,n]) idx, pyx = gda_predictions(X, mus, Sigmas, phis) print(idx) . [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 2 2 2 1 2 1 2 1 2 1 1 1 1 1 1 2 1 1 1 1 1 1 2 1 2 2 2 2 1 1 1 1 1 1 1 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 2 2 1 2 2 2 2 2 2 1 1 2 2 2 2 1 2 1 2 1 2 2 1 1 2 2 2 2 2 1 1 2 2 2 1 2 2 2 1 2 2 2 2 2 2 1] . We visualize the decision boundaries like we did earlier. . from matplotlib.colors import LogNorm xx, yy = np.meshgrid(np.arange(x_min, x_max, .02), np.arange(y_min, y_max, .02)) Z, pyx = gda_predictions(np.c_[xx.ravel(), yy.ravel()], mus, Sigmas, phis) logpy = np.log(-1./3*pyx) # Put the result into a color plot Z = Z.reshape(xx.shape) contours = np.zeros([K, xx.shape[0], xx.shape[1]]) for k in range(K): contours[k] = logpy[k].reshape(xx.shape) plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired) for k in range(K): plt.contour(xx, yy, contours[k], levels=np.logspace(0, 1, 1)) # Plot also the training points plt.scatter(X[:, 0], X[:, 1], c=iris_y, edgecolors=&#39;k&#39;, cmap=plt.cm.Paired) plt.xlabel(&#39;Sepal length&#39;) plt.ylabel(&#39;Sepal width&#39;) plt.show() . Algorithm: Gaussian Discriminant Analysis . Type: Supervised learning (multi-class classification) | Model family: Mixtures of Gaussians. | Objective function: Log-likelihood. | Optimizer: Closed form solution. | . Special Cases of GDA . Many important generative algorithms are special cases of Gaussian Discriminative Analysis . Linear discriminant analysis (LDA): all the covariance matrices $ Sigma_k$ take the same value. | Gaussian Naive Bayes: all the covariance matrices $ Sigma_k$ are diagonal. | Quadratic discriminant analysis (QDA): another term for GDA. | . Generative vs. Discriminative Approaches . Pros of discriminative models: . Often more accurate because they make fewer modeling assumptions. | . Pros of generative models: . Can do more than just prediction: generation, fill-in missing features, etc. | Can include extra prior knowledge; if prior knowledge is correct, model will be more accurate. | Often have closed-form solutions, hence are faster to train. | . $$ || x - y ||_2^2 = (x-y)^ top (x-y) = x^ top x - 2 x^ top y + y^ top y$$ for all $x$ in $X$ and all $y$ in $Y$ .",
            "url": "https://vasudevanubrolu.github.io/erlangenML/2022/02/09/generative-models.html",
            "relUrl": "/2022/02/09/generative-models.html",
            "date": " • Feb 9, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am Vasudev Anubrolu, a 2020 grad in Biological Sciences and Computer Science from BITS Pilani, Pilani Campus. I am a full stack (react, graphql, golang, postgress, dgraph, mongo) engineer building software products at Deloitte Hyderabad. I am working on interesting problems on knowledge graphs, graph databases and DevSecOps. Primarily work in JS, golang and python. I love ML and like to build algorithms that work at scale. .",
          "url": "https://vasudevanubrolu.github.io/erlangenML/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://vasudevanubrolu.github.io/erlangenML/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}